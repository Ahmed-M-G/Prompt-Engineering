{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jOE0yGqJ6Yk9",
        "5G97rtDYDe0e",
        "9WJy6gN_3QF_",
        "WJeBd1Qh3hwo",
        "6FZYyYk83yWc",
        "k6nOsiQ24duO",
        "z5JWPWob4vK0",
        "go236vRm4_vx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up Work Environment"
      ],
      "metadata": {
        "id": "jOE0yGqJ6Yk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AafKYYd-wuUM",
        "outputId": "dbb70d2e-cdd8-4465-be0b-c801afb9e90a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.171.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.72.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai"
      ],
      "metadata": {
        "id": "kvESO7_a1x1d",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google import genai\n",
        "import google.generativeai as ggenai\n",
        "from google.colab import userdata\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "from PIL import Image\n",
        "from google.genai import types\n",
        "\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "qdoxw_y2v5UH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the API key (Replace 'YOUR_API_KEY' with your actual Gemini API key)\n",
        "key = userdata.get('genai_api')\n",
        "client = genai.Client(api_key=key)"
      ],
      "metadata": {
        "id": "dK9gy8xMvlOV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List the set of available models"
      ],
      "metadata": {
        "id": "wR9WQ3B76c3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ggenai.configure(api_key=key)\n",
        "\n",
        "models = ggenai.list_models()\n",
        "for model in models:\n",
        "    print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DKPmEKApyY0Q",
        "outputId": "1b942118-f478-40a2-c6e7-c81c5cd4d136"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/veo-2.0-generate-001\n",
            "models/gemini-2.5-flash-preview-native-audio-dialog\n",
            "models/gemini-2.5-flash-preview-native-audio-dialog-rai-v3\n",
            "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a helper function to make it easier to use prompts and look at generated outputs."
      ],
      "metadata": {
        "id": "P1R9gVXz6e62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gemini-1.5-flash\"):\n",
        "  response = client.models.generate_content(\n",
        "      model=model,\n",
        "      contents=prompt,\n",
        "      config=types.GenerateContentConfig(max_output_tokens=100, temperature=0.1)\n",
        "      )\n",
        "  return response.text  # Extract the generated text"
      ],
      "metadata": {
        "id": "Nho4Ne_Yv_AW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis of Product Review\n",
        "\n",
        "Sentiment analysis is a natural language processing (NLP) technique used to identify and interpret the sentiment expressed in text. It is particularly valuable for analyzing product reviews, enabling businesses to gain insights into customer opinions and feedback. In this example, we’ll apply sentiment analysis to a product review specifically, a review of a lamp. Here’s the review:"
      ],
      "metadata": {
        "id": "5G97rtDYDe0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lamp_review = \"\"\"\n",
        "Needed a nice lamp for my bedroom, and this one had \\\n",
        "additional storage and not too high of a price point. \\\n",
        "Got it fast.  The string to our lamp broke during the \\\n",
        "transit and the company happily sent over a new one. \\\n",
        "Came within a few days as well. It was easy to put \\\n",
        "together.  I had a missing part, so I contacted their \\\n",
        "support and they very quickly got me the missing piece! \\\n",
        "Lumina seems to me to be a great company that cares \\\n",
        "about their customers and products!!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yMI-lygO1G2v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "What is the sentiment of the following product review,\n",
        "which is delimited with triple backticks?\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cHlCvd92wkw",
        "outputId": "8906a8a1-fee5-4ada-f12e-0e0344fdda8c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the review is overwhelmingly **positive**.  The reviewer praises the lamp's features (storage, price), the fast shipping, the company's excellent customer service in resolving the broken string and missing part issues, and expresses overall satisfaction with both the product and the company (Lumina).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "What is the sentiment of the following product review,\n",
        "which is delimited with triple backticks?\n",
        "\n",
        "Give your answer as a single word, either \"positive\" \\\n",
        "or \"negative\".\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL-us1x_2y6M",
        "outputId": "9f7a3666-b807-4401-d085-b038db416175"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify types of emotions\n",
        "\n",
        "Let’s explore another use case where prompt engineering can be applied to perform a more advanced sentiment analysis task emotion detection. Large language models are highly effective at extracting specific information from text, including identifying the emotions expressed. This can be especially useful for understanding how customers feel about a product. For customer support teams, in particular, detecting whether a user is extremely upset can be critical for prioritizing responses.\n",
        "\n",
        "Below is a sample prompt designed to extract emotional cues from customer feedback:"
      ],
      "metadata": {
        "id": "9WJy6gN_3QF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Identify a list of emotions that the writer of the \\\n",
        "following review is expressing. Include no more than \\\n",
        "five items in the list. Format your answer as a list of \\\n",
        "lower-case words separated by commas.\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INry1v_A3Ltq",
        "outputId": "dba45ffa-d190-4b5e-977d-5661a3f289c1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "satisfaction, gratitude, relief, happiness, trust\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify Anger\n",
        "\n",
        "In some cases, your classification task may focus on detecting a specific emotion such as identifying whether a customer is angry. Knowing when a customer expresses strong negative emotions can be critical. For instance, if someone is clearly angry, it may warrant special attention from customer support or customer success teams, who can reach out proactively to understand the issue and work toward a resolution."
      ],
      "metadata": {
        "id": "WJeBd1Qh3hwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Is the writer of the following review expressing anger?\\\n",
        "The review is delimited with triple backticks. \\\n",
        "Give your answer as either yes or no.\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WLqUorY3bkH",
        "outputId": "3afa8ff9-8773-461b-c566-400e6db6522d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Product & Company Names From Customer Reviews\n",
        "\n",
        "Information extraction is a key task in natural language processing (NLP) that involves identifying and retrieving specific pieces of information from a given text. In the following prompt, we’ll ask the language model to extract two specific details: the item that was purchased and the name of the company that manufactured it."
      ],
      "metadata": {
        "id": "6FZYyYk83yWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Identify the following items from the review text:\n",
        "- Item purchased by reviewer\n",
        "- Company that made the item\n",
        "\n",
        "The review is delimited with triple backticks. \\\n",
        "Format your response as a JSON object with \\\n",
        "\"Item\" and \"Brand\" as the keys.\n",
        "If the information isn't present, use \"unknown\" \\\n",
        "as the value.\n",
        "Make your response as short as possible. \\\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0KjwXJX3tAL",
        "outputId": "d9548f41-6f90-429e-b464-b784fd561158"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"Item\": \"lamp\",\n",
            "  \"Brand\": \"Lumina\"\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doing Multiple Tasks at Once\n",
        "\n",
        "In the examples we've explored, we created prompts to detect sentiment, determine if a reviewer is angry, and extract both the purchased item and its brand.\n",
        "\n",
        "One approach to extract all this information is to use multiple prompts calling the `get_completion` function three or four times, once for each piece of information. However, a more efficient method is to combine everything into a single prompt that captures all the desired outputs at once.\n",
        "\n",
        "For example, you can instruct the model to:\n",
        "\"Identify the following: sentiment, whether the reviewer is expressing anger, the item purchased, and the company that made it.\"\n",
        "You can also specify the format—for instance, requesting the anger value as a Boolean (`true` or `false`)."
      ],
      "metadata": {
        "id": "k6nOsiQ24duO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Identify the following items from the review text:\n",
        "- Sentiment (positive or negative)\n",
        "- Is the reviewer expressing anger? (true or false)\n",
        "- Item purchased by reviewer\n",
        "- Company that made the item\n",
        "\n",
        "The review is delimited with triple backticks. \\\n",
        "Format your response as a JSON object with \\\n",
        "\"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys.\n",
        "If the information isn't present, use \"unknown\" \\\n",
        "as the value.\n",
        "Make your response as short as possible.\n",
        "Format the Anger value as a boolean.\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGbcnBWA39UB",
        "outputId": "ad77af7f-f85e-4637-80f3-382863d79aa5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"Sentiment\": \"positive\",\n",
            "  \"Anger\": false,\n",
            "  \"Item\": \"lamp\",\n",
            "  \"Brand\": \"Lumina\"\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topics Inferring\n",
        "\n",
        "Another advanced NLP task you can perform is topic inference determining the main themes or subjects within a piece of text. Given a long article or document, the goal is to understand what it's about and what topics are being discussed.\n",
        "\n",
        "For example, consider this fictitious newspaper article summarizing a recent government survey about how agency employees feel about their workplace."
      ],
      "metadata": {
        "id": "z5JWPWob4vK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "story = \"\"\"\n",
        "In a recent survey conducted by the government,\n",
        "public sector employees were asked to rate their level\n",
        "of satisfaction with the department they work at.\n",
        "The results revealed that NASA was the most popular\n",
        "department with a satisfaction rating of 95%.\n",
        "\n",
        "One NASA employee, John Smith, commented on the findings,\n",
        "stating, \"I'm not surprised that NASA came out on top.\n",
        "It's a great place to work with amazing people and\n",
        "incredible opportunities. I'm proud to be a part of\n",
        "such an innovative organization.\"\n",
        "\n",
        "The results were also welcomed by NASA's management team,\n",
        "with Director Tom Johnson stating, \"We are thrilled to\n",
        "hear that our employees are satisfied with their work at NASA.\n",
        "We have a talented and dedicated team who work tirelessly\n",
        "to achieve our goals, and it's fantastic to see that their\n",
        "hard work is paying off.\"\n",
        "\n",
        "The survey also revealed that the\n",
        "Social Security Administration had the lowest satisfaction\n",
        "rating, with only 45% of employees indicating they were\n",
        "satisfied with their job. The government has pledged to\n",
        "address the concerns raised by employees in the survey and\n",
        "work towards improving job satisfaction across all departments.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "klkGIaTw4sYz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Determine five topics that are being discussed in the \\\n",
        "following text, which is delimited by triple backticks.\n",
        "\n",
        "Make each item one or two words long.\n",
        "\n",
        "Format your response as a list of items separated by commas.\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3FDTSjr44io",
        "outputId": "672b73ab-bc3b-4e38-9bd3-dcf9ac92f2aa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee satisfaction, NASA, Government survey, Job satisfaction, Social Security\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.split(sep=',')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdAgwEr45Rym",
        "outputId": "c2f8102c-8280-4366-c5bd-2496c60b273e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Employee satisfaction',\n",
              " ' NASA',\n",
              " ' Government survey',\n",
              " ' Job satisfaction',\n",
              " ' Social Security\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Alert for Certain Topics\n",
        "\n",
        "If you have a collection of articles and extract topics from each, you can leverage a large language model to help organize and index the content by topic. This makes it easier to navigate and retrieve information based on specific themes. For this example, let’s work with a slightly different set of topics to illustrate how this can be done."
      ],
      "metadata": {
        "id": "go236vRm4_vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_list = [\n",
        "    \"nasa\", \"local government\", \"engineering\",\n",
        "    \"employee satisfaction\", \"federal government\"\n",
        "]"
      ],
      "metadata": {
        "id": "zlAFsu1E46PL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Determine whether each item in the following list of \\\n",
        "topics is a topic in the text below, which\n",
        "is delimited with triple backticks.\n",
        "\n",
        "Give your answer as follows:\n",
        "item from the list: 0 or 1\n",
        "\n",
        "List of topics: {\", \".join(topic_list)}\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQv95ZCQ5NQj",
        "outputId": "3230c5f7-a856-4d4f-c448-98072dc053ba"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nasa: 1\n",
            "local government: 0\n",
            "engineering: 0\n",
            "employee satisfaction: 1\n",
            "federal government: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that each item can be split into at least two parts before creating the dictionary\n",
        "topic_dict = {i.split(': ')[0]: int(i.split(': ')[1]) for i in response.split(sep='\\n') if len(i.split(': ')) >= 2}\n",
        "if 'nasa' in topic_dict and topic_dict['nasa'] == 1:\n",
        "    print(\"ALERT: New NASA story!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5d312zW5XJ6",
        "outputId": "1f875f28-745c-4b33-ab99-c4fe10b46625"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALERT: New NASA story!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Determine whether each item in the following list of \\\n",
        "topics is a topic in the text below, which\n",
        "is delimited with triple backticks.\n",
        "\n",
        "Give your answer as follows:\n",
        "item from the list: 0 or 1 and put all the results in a dictionary format\n",
        "\n",
        "List of topics: {\", \".join(topic_list)}\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFHzpODL5av3",
        "outputId": "59b6fd9c-b3c6-45e6-8d87-945d5c24bc98"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"nasa\": 1,\n",
            "  \"local government\": 0,\n",
            "  \"engineering\": 0,\n",
            "  \"employee satisfaction\": 1,\n",
            "  \"federal government\": 1\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OB6L76Td6H0V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}